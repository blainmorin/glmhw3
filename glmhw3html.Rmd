---
title: 'PHP 2514 Homework #3'
author: "Blain Morin"
date: "March 19, 2018"
output: 
  html_document:
    theme: journal
---

#Question 1:

Assume X is in units of $10,000.

First, find y-intercept ($\beta_0$):

$$
\begin{align}
\pi &= .27\\
x &= 0\\
logit(.27) &= \ln(\frac{.27}{1-.27}) = \beta_0 + \beta_1X\\
-.9946 &= \beta_0
\end{align}
$$

Next, find the slope ($\beta_1$):

$$
\begin{align}
\pi &= .88 \\
x &= 6 \\
logit(.88) &=\ln(\frac{.88}{1-.88}) = -.9946 +\beta_1 *6\\
.4978 &= \beta_1
\end{align}
$$

Thus, the equation is:

$$
\pi = logit^{-1}(-.9946 + .4978X)
$$

#Question 2

##a.)
```{r, message=FALSE, warning=FALSE}

library(tidyverse)
library(arm)

###create data set
studentid = 1:50
set.seed(50)
grades = rnorm(50, 60, 15)

###use the our logit to predict if a student passed using their midterm grade
prob.pass = c()
for (i in 1:50) {
  
  prob = invlogit(-24 + .4 * grades[i])
  prob.pass[i] = rbinom(1, 1, prob = prob)
  
}

classdata = data.frame(studentid, grades, prob.pass)



###graph our points with the logit function given by the problem
classdata %>%
  ggplot(aes(x = grades, y = prob.pass)) + 
    stat_function(fun = function(x) invlogit(-24 + .4*x), color = "aquamarine3", size = 2) +
    geom_point(size = 3.5, alpha = .4, color = "coral3") + 
    theme_minimal() +
    xlab("Midterm Grade") +
    ylab("Pr(pass)") +
    ggtitle("Passed class vs Midterm Grade")
  
```

##b.)

```{r}

###Add a column with standardized grades
classdata = classdata %>%
  mutate(stand.grade = (grades - mean(grades)) / sd(grades))


###Plot our standardized data with our updated logit curve
###To get our new logit function we multiplied the B0 by 0
###and multiplied B1 by 15
classdata %>%
    ggplot(aes(x = stand.grade, y = prob.pass)) + 
    stat_function(fun = function(x) invlogit((-24*0) + (.4*15)*x), color = "aquamarine3", size = 2) +
    geom_point(size = 3.5, alpha = .4, color = "coral3") + 
    theme_minimal() +
    xlab("Standardized Grade") +
    ylab("Pr(pass)") +
    ggtitle("Pr(Pass) vs Standardized Grade")

```


##c.)

```{r, message = FALSE, warning=FALSE}
library(sjPlot)
attach(classdata)

###Add random noise column to our data
noise = rnorm(50, 0 , 1)
classdata$noise = noise

###Run model with no noise
model.no.noise = glm(prob.pass ~ stand.grade, family = binomial(link = "logit"))


###Run model with noise
model.noise = glm(prob.pass ~ stand.grade + noise, family = binomial(link = "logit"))


print(c(model.no.noise$deviance, model.noise$deviance))

detach(classdata)

```

The deviance in our noise model is very similar to the deviance in the first model (20.93 vs 20.74. This makes sense theoretically: since the noise is randomly generated, it shouldn't impact the fit of our model (except for some random spurious correlation). Thus, our residuals will not change much. Therefore, the deviance will not change much. 


#Question 3


##a.)

First we examined the histogram for our race categories. 

```{r}

rodents = read.table("rodents.txt")
rodents$rodent2 = as.factor(rodents$rodent2)
rodents$race = as.factor(rodents$race)


hist(as.numeric(rodents$race))
```

We see that race indicators 6 and 7 have a low frequency, so we will bin those together: 

```{r}

###Combine race 6 and 7 into category 6. 

race.edit = c()

for (i in 1:nrow(rodents)) {
  if (rodents$race[i] == 7) {
    race.edit[i] = 6
  } else {
    race.edit[i] = rodents$race[i]
  }
}

rodents$race.edit = as.factor(race.edit)

base.model = glm(formula = rodent2 ~ race.edit,
                 family = binomial(link = "logit"), 
                 data = rodents)

```

```{r, include = FALSE}

summary(base.model)

stargazer::stargazer(base.model, type = "html")

```

<table style="text-align:center"><tr><td colspan="2" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td><em>Dependent variable:</em></td></tr>
<tr><td></td><td colspan="1" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td>rodent2</td></tr>
<tr><td colspan="2" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">race.edit2</td><td>1.536<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.169)</td></tr>
<tr><td style="text-align:left"></td><td></td></tr>
<tr><td style="text-align:left">race.edit3</td><td>1.449<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.214)</td></tr>
<tr><td style="text-align:left"></td><td></td></tr>
<tr><td style="text-align:left">race.edit4</td><td>1.867<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.187)</td></tr>
<tr><td style="text-align:left"></td><td></td></tr>
<tr><td style="text-align:left">race.edit5</td><td>0.400</td></tr>
<tr><td style="text-align:left"></td><td>(0.292)</td></tr>
<tr><td style="text-align:left"></td><td></td></tr>
<tr><td style="text-align:left">race.edit6</td><td>1.364<sup>**</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.554)</td></tr>
<tr><td style="text-align:left"></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>-2.152<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.128)</td></tr>
<tr><td style="text-align:left"></td><td></td></tr>
<tr><td colspan="2" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>1,522</td></tr>
<tr><td style="text-align:left">Log Likelihood</td><td>-760.124</td></tr>
<tr><td style="text-align:left">Akaike Inf. Crit.</td><td>1,532.248</td></tr>
<tr><td colspan="2" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>

In the output above, race 1 is our reference category. Raising e to the beta is the multiplicitive change in the odds of having rodents in the apartment. For example, for race category 2: 

```{r}

exp(1.536)

```

This is interpretted as: 

Compared to race category 1, the odds of having rodents in the apartment is 4.6 times higher for those people in race category 2. 

##b.)

Next, we try to find a good model using other other house-hold level variables. The variables that I'm choosing to examine are:

- Rotted / Broken Windows
- Missing or Worn Flooring
- Cracks in Walls
- Holes in the flooring
- If the building was built before 1947
- If there is a regular exterminator
- Total household income
- Housing type (rent, public housing, owned)

```{r, message=FALSE, results="hide"}

attach(rodents)

b.data = data.frame(
  as.factor(rodent2),
  as.factor(extwin4_2),
  as.factor(extflr5_2),
  as.factor(intcrack2),
  as.factor(inthole2),
  as.factor(old),
  as.factor(race.edit),
  as.factor(regext),
  totincom2
)

detach(rodents)

attach(b.data)

###change income units to thousand
b.data$totincom2 = b.data$totincom2/10000

###remove NAs
b.data = na.omit(b.data)

b.model = glm(as.factor.rodent2. ~ ., family = binomial(link="logit"), data = b.data)

b.step.model = step(b.model, direction = "both")

```

```{r, echo=FALSE}
sjt.glm(b.step.model)
```

The odds ratios for all of the variables are given in the above table. The interpretation of these is similar to part(a). For example, having a hole in the floor of the apartment means that there is 2.88 times the odds of having rodents than in an apartment with no holes in the floor. The interpretation is similar for income: a household earning 1000 dollars more than another similar household has .95 times the odds of having rodents. 

Overall, most of the variables in our model are significant at the 5 percent level. The signs of the household factors are consistent with what one would expect (ie holes in the floor is associated with higher odds of having rats). The variables that the model dropped were having a regular exterminator and having a crack in the exterior windows. 

Next we checked the fit using residual diagnostics. First, we compared the residuals and fitted values:
```{r}

plot(fitted(b.step.model), resid(b.step.model))
lines(lowess(predict(b.step.model),residuals(b.step.model)),col="black",lwd=2)

```

Since this is a logistic regression, we see two groups of points. The black line is a regression on the points. We see that this line is fairly close to the 0 line, which means that there is no pattern in our residuals. We can examine the residuals further using a bin plot:

```{r, warning=FALSE}

library(arm)
binnedplot(predict.glm(b.step.model), resid(b.step.model))

```

The grey lines represent the 95% confidence interval for the residuals assuming that the model was true. We see that the model is mostly okay, but that there are a few point in the lower expected values that fall outside of the confidence interval. This could be a problem with the model. We would maybe need to add a quadratic term or use a log transformation. 

We also looked at the ROC curve:

```{r, warning=FALSE}

library(ModelGood)
plot(Roc(b.step.model), auc = TRUE)


```

As seen above, the area under our ROC curve is .77. This means that our model does a decent job of discriminating between apartments with and without rodents. 

Next we looked at a calibration curve for our model:

```{r}

calPlot2(b.step.model)

```

We can see from the calibration plot that our model predicts the observed probability closely up to probabilities of .6. Above .6, our model underestimates. 


#Question 4

For our first model, we use only the clinical variables. Since our outcome (dehydration) is an ordinal factor, we use a cumulative logit model. 

```{r, message=FALSE, warning=FALSE}

library(readr)
library(nnet)
library(ordinal)
library(sjPlot)

###Select clinical only columns
dhaka = read_csv("dhaka.csv")
clinicaldata = dhaka[, 1:13]

###Change columns to factors
clinicaldata = lapply(clinicaldata, as.factor)
clinicaldata = as.data.frame(clinicaldata)

###Remove NA
clinicaldata = na.omit(clinicaldata)

###Run regression for only clinical predictors
model.clinical = clm(ordered(dehyd) ~ capref +
                        extrem +
                        eyes +
                        genapp +
                        mucous +
                        pulse +
                        resp + 
                        skin +
                        tears +
                        thirst +
                        urine +
                        heart,
                      data = clinicaldata)

summary(model.clinical)

```

We can see from the above output that there is something peculiar going on with our standard errors and p values (they are all NA). Examining the data further sheds light onto whats happening:

```{r}

summary(clinicaldata)

```

We can see from the data summary above the the variables capref, heart, and mucous have small responses. For example, there is only one person with capref = 1. This causes our standard errors to approach infinity and causes R to return NA. We can fix this by removing the offending variables:

```{r}

###Run regression omitting low response variables
model.clinical2 = clm(ordered(dehyd) ~ 
                        extrem +
                        eyes +
                        genapp +
                        pulse +
                        resp + 
                        skin +
                        tears +
                        thirst +
                        urine,
                      data = clinicaldata)

summary(model.clinical2)

```

We see that the NA problem is fixed. We see that eyes, general appearance, skin and tears are significant at the 5% level. Except for thirst, the other variables have the expected sign. Thus, the only variable we will exclude from our next model is thirst. 

Next we will add the individual level variables to the model:

```{r, results="hide"}

library(dplyr)

###Merge factor data with nonfactor variables
clinicaldata = dhaka[, 1:13]
clinicaldata = lapply(clinicaldata, as.factor)
clinicaldata = as.data.frame(clinicaldata)
nonclinical = dhaka[,14:20]
nonclinical$female = as.factor(nonclinical$female)
dhaka.clean = cbind(clinicaldata, nonclinical)

###Remove heart, mucous, capref columns (due to nonresponse)

dhaka.clean = dhaka.clean %>%
  dplyr::select(-heart, -capref, -mucous, -thirst)

###Remove NAs
dhaka.clean = na.omit(dhaka.clean)

###Specify cumulative formula
allmodel = clm(ordered(dehyd) ~ .,
                      data = dhaka.clean)

###Use step algorithm to find best model
best.dhaka = step(allmodel, direction = "both")
  
```

Here is the summary of the model with the best AIC score:

```{r}
summary(best.dhaka)
```

Our final model uses general apearance, skin pinch, tears, age, episodes, and mid upper arm circumfrence. The sign on all the coefficients are consistent with what we would expect. The reference catagory is dehyd = 0.

The betas for the factor variables can be interpretted in terms of odds ratios as follows:

When a child has a restless appearance (genapp1 = 1), the odds of some dehydration (dehyd = 1) is 2.2 times higher (exp(.7995)) than that of a normal appearance child.

Moreover, when a child has a restless appearance (genapp = 1), the odds of severe dehydration (dehyd = 2) is 3.97 times higher (exp(1.3791)) than that of a normal appearance child.

The betas for the nominal variables can be interpretted as follows:

An additional month of age increase the odds of being dehydrated by a factor of 1.03 (exp(.0314)).